# Data-Mining-Specialization
 Data mining techniques for both structured data which conform to a clearly defined schema, and unstructured data which exist in the form of natural language text. Specific course topics include pattern discovery, clustering, text retrieval, text mining and analytics, and data visualization. The Capstone project task is to solve real-world data mining challenges using a restaurant review data set from Yelp.
 
 Data Mining Specialization is offered by University of Illinois at Urbana-Champaign on Coursera.
 
 ## Course 1: Data Mining
 #### Week 1: The Computer and the Human
* Introduction to visualization
* Using computer graphics to display data
* The model human processor and Fitts's law
* Human visual perception and cognition
 
 #### Week 2: Visualization of Numerical Data
* Different kinds of visualizations and how best to apply them to data
* Basic charts such as bar charts and scatter plots
* More advanced visualization techniques, such as streamgraphs and parallel coordinates
* Some elements of design and color usage

 #### Week 3: Visualization of Non-Numerical Data
* Graphs, networks, and hierarchies
* Layout of relational and hierarchical data, such as treemaps
* Methods for visualizing high-dimensional data, such as principal component analysis and multidimensional scaling 
 
 #### Week 4: The Visualization Dashboard
* Visualizing large datasets
* Visualization of databases and data mining results
* Visual analytics for decision support
* Task analysis
* Visualization dashboards
 
 ## Course 2: Text Retrieval and Search Engines
 #### Week 1:
* Part of Speech tagging, syntactic analysis, semantic analysis, and ambiguity
* “Bag of words” representation
* Push, pull, querying, browsing
* Probability ranking principle
* Relevance
* Vector space model
* Dot product
* Bit vector representation
 #### Week 2:
* Term frequency (TF)
* Document frequency (DF) and inverse document frequency (IDF)
* TF transformation
* Pivoted length normalization
* BM25
* Inverted index and postings
* Binary coding, unary coding, gamma-coding, and d-gap
* Zipf’s law

 #### Week 3:
* Cranfield evaluation methodology
* Precision and recall
* Average precision, mean average precision (MAP), and geometric mean average precision (gMAP)
* Reciprocal rank and mean reciprocal rank
* F-measure
* Normalized Discounted Cumulative Gain (nDCG)
* Statistical significance test 

 #### Week 4:
* p(R=1|q,d), query likelihood, and p(q|d)
* Statistical and unigram language models
* Maximum likelihood estimate
* Background, collection, and document language models
* Smoothing of unigram language models
* Relation between query likelihood and TF-IDF weighting
* Linear interpolation (i.e., Jelinek-Mercer) smoothing
* Dirichlet Prior smoothing
 
 #### Week 5:
* Relevance feedback
* Pseudo-relevance feedback
* Implicit feedback
* Rocchio feedback
* Kullback-Leiber divergence (KL-divergence) retrieval function
* Mixture language model
* Scalability and efficiency
* Spams
* Crawler, focused crawling, and incremental crawling
* Google File System (GFS)
* MapReduce
* Link analysis and anchor text
* PageRank and HITS

 #### Week 6:
* Learning to rank, features, and logistic regression
* Content-based filtering
* Collaborative filtering
* Beta-Gamma threshold learning
* Linear utility
* User profile
* Exploration-exploitation tradeoff
* Memory-based collaborative filtering
* Cold start

 ## Course 3: Text Mining and Analytics
 #### Week 1:
* Part of speech tagging
* Syntactic analysis
* Semantic analysis
* Ambiguity
* Text representation, especially bag-of-words representation
* Context of a word; context similarity
* Paradigmatic relation
* Syntagmatic relation

 #### Week 2:
* Entropy
* Conditional entropy
* Mutual information
* Topic and coverage of topic
* Language model
* Generative model
* Unigram language model
* Word distribution
* Background language model
* Parameters of a probabilistic model
* Likelihood
* Bayes rule
* Maximum likelihood estimation
* Prior and posterior distributions
* Bayesian estimation & inference
* Maximum a posteriori (MAP) estimate
* Prior model
* Posterior mode

 #### Week 3:
* Mixture model
* Component model
* Constraints on probabilities
* Probabilistic Latent Semantic Analysis (PLSA)
* Expectation-Maximization (EM) algorithm
* E-step and M-step
* Hidden variables
* Hill climbing
* Local maximum
* Latent Dirichlet Allocation (LDA)
 
 #### Week 4:
* Clustering, document clustering, and term clustering
* Clustering bias
* Perspective of similarity
* Mixture model, likelihood, and maximum likelihood estimation
* EM algorithm, E-step, M-step, underflow, normalization (to avoid underflow)
* Hierarchical Agglomerative Clustering, and k-Means
* Direction evaluation (of clustering), indirect evaluation (of clustering)
* Text categorization, topic categorization, sentiment categorization, email routing
* Spam filtering
* Naïve Bayes classifier
* Smoothing

 #### Week 5:
* Generative classifier vs. discriminative classifier
* Training data
* Logistic regression
* K-Nearest Neighbor classifier
* Support Vector Machine (SVM), margin, and linear separator
* Classification accuracy, precision, recall, F measure, macro-averaging, and micro-averaging
* Opinion holder, opinion target, sentiment, opinion representation
* Sentiment classification
* Features, n-grams, frequent patterns, and overfitting
* Ordinal logistic regression
* Rating prediction

 #### Week 6:
* Aspect rating and aspect weight
* Latent aspect rating analysis (LARA)
* Latent rating regression model
* Generative model
* Rating prediction
* Normal/Gaussian distribution
* Prior vs. posterior probability
* Text-based prediction
* The “data mining loop”
* Context (of text data) and contextual text mining
* Contextual probabilistic latent semantic analysis (CPLSA): views of a topic and coverage of topics
* Spatiotemporal trends of topics
* Event impact analysis
* Network-regularized topic modeling
* NetPLSA
* Causal topics
* Iterative topic modeling with time series supervision

 ## Course 4: Pattern Discovery in Data Mining
 #### Week 1.1: Pattern Discovery Basic Concepts 
 #### Week 1.2: Efficient Pattern Mining Methods
 
 #### Week 2.1: Pattern Evaluation 
 #### Week 2.2: Mining Diverse Frequent Patterns
 
 #### Week 3.1: Sequential Pattern Mining 
 #### Week 3.2: Pattern Mining Applications: Mining Spatiotemporal and Trajectory Patterns
 
 #### Week 4.1: Pattern Mining Applications: Mining Quality Phrases from Text Data
 #### Week 4.2: Advanced Topics on Pattern Discovery
 
 ## Course 5: Cluster Analysis in Data Mining
 #### Week 1: Cluster Analysis & Measures
 
 #### Week 2.1: Partitioning-Based Clustering Methods 
 #### Week 2.2: Hierarchical Clustering Methods 
 
 #### Week 3: Density-Based and Grid-Based Clustering Methods 
 
 #### Week 4: Methods for Clustering Validation
  
 ## Capstone: Data Mining Project
 #### Task 1: Exploration of a Data Set
 #### Task 2: Cuisine Clustering and Map Construction
 #### Task 3: Dish Recognition
 #### Task 4&5: Popular Dishes and Retaurant Recommendation
 #### Task 6: Prediction whether a set of restaurants will pass the public health inspection test Based on Yelp Reviews
 
